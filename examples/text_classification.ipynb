{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "Copy of text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL60gfOrcILF"
      },
      "source": [
        "%%capture\n",
        "!pip install https://3358-217161669-gh.circle-artifacts.com/0/wheels/torch-1.7.0a0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install https://3358-217161669-gh.circle-artifacts.com/0/wheels/nestedtensor-0.0.1.dev20201167-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s1tLazJct6y"
      },
      "source": [
        "import re\n",
        "import requests\n",
        "import io\n",
        "import tarfile\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import sys\n",
        "import concurrent.futures\n",
        "import time\n",
        "from collections import Counter\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import nestedtensor\n",
        "\n",
        "URL = \"https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg3h4hOJcILJ"
      },
      "source": [
        "Point = namedtuple('Point', 'label text')\n",
        "\n",
        "def get_data(URL):\n",
        "    r = requests.get(URL)\n",
        "    file_like_object = io.BytesIO(r.content)\n",
        "    tar = tarfile.open(fileobj=file_like_object)\n",
        "    d = {}\n",
        "    for member in tar.getmembers():\n",
        "        if member.isfile() and member.name.endswith('csv'):\n",
        "            k = 'train' if 'train' in member.name else 'test'\n",
        "            d[k] = tar.extractfile(member)\n",
        "    return d\n",
        "\n",
        "\n",
        "def preprocess(iterator):\n",
        "    def _preprocess(line):\n",
        "        line = line.decode('UTF-8')\n",
        "        line = line.lower()\n",
        "        line = re.sub(r'[^0-9a-zA-Z,\\s]', \"\", line)\n",
        "        line = line.split(',')\n",
        "        label = int(line[0]) - 1\n",
        "        text = (\" \".join(line[1:])).split()\n",
        "        if len(line) > 2:\n",
        "            return Point(label=label, text=text)\n",
        "    for line in iterator:\n",
        "        yield _preprocess(line)\n",
        "\n",
        "\n",
        "def build_vocab(iterator):\n",
        "    counter = Counter()\n",
        "    labels = set()\n",
        "    for point in iterator:\n",
        "        counter.update(point.text)\n",
        "        labels.add(point.label)\n",
        "    vocab = {}\n",
        "    for i, (word, count) in enumerate(counter.most_common()):\n",
        "        vocab[word] = i\n",
        "\n",
        "    return vocab, labels"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_G7pmJUcILL"
      },
      "source": [
        "data = get_data(URL)\n",
        "data = {k: list(preprocess(v)) for (k, v) in data.items()}\n",
        "vocab, labels = build_vocab(data['train'])\n",
        "UNK = len(vocab)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-2xf24FcILO"
      },
      "source": [
        "class TextSentiment(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text):\n",
        "        return self.fc(self.embedding(text))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fS1Eo9jcILQ"
      },
      "source": [
        "embed_dim = 10\n",
        "model = TextSentiment(len(vocab) + 1, embed_dim, len(labels))\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 0.1, gamma=0.9)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob4HSMOMcILT"
      },
      "source": [
        "def create_batch(data):\n",
        "    data = nestedtensor.nested_tensor(\n",
        "        [torch.tensor(list(map(lambda x: vocab.get(x, UNK), tokens))) for tokens in data], dtype=torch.int64)\n",
        "    return data\n",
        "\n",
        "def yield_data_futures(data):\n",
        "    random.shuffle(data)\n",
        "    labels = []\n",
        "    batch_data = []\n",
        "    futures = []\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=1) as executor:\n",
        "        for i, point in enumerate(data):\n",
        "            # Stop accumulating lines of text once we reach 4000 tokens or more\n",
        "            # This yields variable batch sizes, but with consistent memory pressure\n",
        "            if sum(map(len, batch_data), 0) < 10000:\n",
        "                labels.append(torch.tensor(point.label))\n",
        "                batch_data.append(point.text)\n",
        "            else:\n",
        "                if len(futures) < 40:\n",
        "                    futures.append((nestedtensor.nested_tensor(labels, dtype=torch.int64), create_batch(batch_data)))\n",
        "                else:\n",
        "                    yield futures[0]\n",
        "                    futures = futures[1:]\n",
        "                labels = []\n",
        "                batch_data = []\n",
        "\n",
        "    for future in futures:\n",
        "        yield future"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBZLiRh2cILV",
        "outputId": "88ff15d9-2135-4cb0-889a-c51f52f9ce9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_tokens = sum(map(lambda x: len(x.text), data['train']))\n",
        "print(\"Total number of tokens: {}\".format(num_tokens))\n",
        "for epoch in range(5):\n",
        "    i = 0\n",
        "    t0 = time.time()\n",
        "    for labels, future in yield_data_futures(data['train']):\n",
        "        batch = future\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        loss = criterion(output, labels).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 16 == 1:\n",
        "            sys.stderr.write(\n",
        "                \"\\rtime: {:3.0f}s epoch: {:3.0f} lr: {:3.6f} loss: {:3.6f}\".format(\n",
        "                    time.time() - t0, \n",
        "                    epoch, \n",
        "                    scheduler.get_last_lr()[0],\n",
        "                    loss, \n",
        "                )\n",
        "            )\n",
        "            sys.stderr.flush()\n",
        "        i += batch.numel()\n",
        "    scheduler.step()\n",
        "    sys.stderr.write('\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of tokens: 27205880\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "time: 298s epoch:   0 lr: 1.000000 loss: 0.722789\n",
            "time: 303s epoch:   1 lr: 1.000000 loss: 0.435096\n",
            "time: 302s epoch:   2 lr: 1.000000 loss: 0.225737\n",
            "time: 297s epoch:   3 lr: 1.000000 loss: 0.173856\n",
            "time: 299s epoch:   4 lr: 1.000000 loss: 0.123066\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v05_XWkYcILY",
        "outputId": "04188cf4-2b9f-46a8-c0e0-3d49b7c574bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_correct = 0\n",
        "total_num = 0\n",
        "for tb in yield_data_futures(data['test']):\n",
        "  output = model(tb[1]).to_tensor().argmax(1)\n",
        "  num_correct += (tb[0].to_tensor() == output).sum().item()\n",
        "  total_num += len(target)\n",
        "\n",
        "print(\"Test accuracy: {}\".format(float(num_correct) / float(total_num)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.9435707678075855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PT0PxhFcILa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}